{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Sentiment Analysis on YouTube Channel Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import isodate\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('gaugegadget.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Drop columns that are not much of value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['favouriteCount','video_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Convert publishedAt and duration to a better readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publishedAt'] = pd.to_datetime(data['publishedAt'])\n",
    "def convert_duration(duration_str):\n",
    "    duration = isodate.parse_duration(duration_str)\n",
    "    return duration.total_seconds() / 60\n",
    "\n",
    "data['duration'] = data['duration'].apply(convert_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='duration', ascending=False, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Define a function for cleaning the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    # Remove numbers\n",
    "    #text = re.sub(r'\\d+','',text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "text = 'The quick brown foxES aRe jumping ove@#$r the la#$zy dogs!'\n",
    "print(process_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset='tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed_tags'] = data['tags'].apply(process_text)\n",
    "data['processed_description'] = data['description'].apply(process_text)\n",
    "data['processed_title'] = data['title'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = ' '.join(data['processed_tags'].dropna())\n",
    "all_title = ' '.join(data['processed_title'].dropna())\n",
    "all_description = ' '.join(data['processed_description'].dropna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "g_path = 'Gletter.png'  # Replace with the path to your mask image\n",
    "t_path = 'T.png'\n",
    "d_path = 'Dletter.png'\n",
    "g_image = Image.open(g_path)\n",
    "t_image = Image.open(t_path)\n",
    "d_image = Image.open(d_path)\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "g_array = np.array(g_image)\n",
    "t_array = np.array(t_image)\n",
    "d_array = np.array(d_image)\n",
    "\n",
    "# Create a WordCloud object with the mask\n",
    "gcloud = WordCloud(width=800, height=400, background_color='white', max_words=200, mask=g_array, contour_width=0, contour_color='black', min_font_size=10).generate(all_tags)\n",
    "tcloud = WordCloud(width=800, height=400, background_color='white', max_words=200, mask=t_array, contour_width=0, contour_color='black', min_font_size=10).generate(all_title)\n",
    "dcloud = WordCloud(width=800, height=400, background_color='white', mask=d_array, contour_width=0, contour_color='black').generate(all_description)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(gcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Words in Tags')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(tcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Words in Title')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(dcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Words in Description')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Plotting some visuals to gain insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values or fill them with appropriate values\n",
    "data = data.dropna(subset=['publishedAt', 'viewCount', 'title'])\n",
    "\n",
    "# Convert to datetime and numeric, if not already done\n",
    "data['publishedAt'] = pd.to_datetime(data['publishedAt'], errors='coerce')\n",
    "data['viewCount'] = pd.to_numeric(data['viewCount'], errors='coerce')\n",
    "\n",
    "# Extract month and year from the date and create a new column for aggregation\n",
    "data['month_year'] = data['publishedAt'].dt.to_period('M')\n",
    "\n",
    "# Define a custom aggregation function to get the titles of the top 3 videos with the highest view counts\n",
    "def top_titles(series):\n",
    "    top_n = 3  # You can change this to get the top N titles\n",
    "    sorted_series = series.sort_values(ascending=False)\n",
    "    top_titles = sorted_series.head(top_n).index.tolist()\n",
    "    return '<br>'.join(top_titles)\n",
    "\n",
    "# Group by 'month_year', sum the 'viewCount', and get the top titles\n",
    "data_grouped = data.groupby('month_year').agg({\n",
    "    'viewCount': 'sum',\n",
    "    'title': lambda x: top_titles(x.groupby(x).count())\n",
    "}).reset_index()\n",
    "\n",
    "# Create an interactive line plot\n",
    "fig = px.line(data_grouped, x=data_grouped['month_year'].astype(str), y='viewCount', \n",
    "              title='Monthly Views Trend', labels={'viewCount': 'Total Views', 'month_year': 'Month-Year'},\n",
    "              hover_data={'title': True})  # Include 'title' in hover data\n",
    "\n",
    "# Customize hover data\n",
    "fig.update_traces(mode='lines+markers',\n",
    "                  hovertemplate='Month-Year: %{x}<br>Total Views: %{y:.2f}<br>Top Titles: %{customdata[0]}')\n",
    "\n",
    "fig.update_layout(xaxis_title='Month-Year')\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "pyo.plot(fig, filename='line.html', config={'displayModeBar': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['viewCount', 'likeCount', 'commentCount']\n",
    "sns.pairplot(data[numerical_features])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='viewCount', y='likeCount', data=data)\n",
    "plt.title('Scatter plot between View Count and Like Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
